---
layout: post
read_time: true
show_date: true
title:  TIL 9
date:   2021-12-20 23:00
tags: [Til]
author: 이대겸
github:  
mathjax: yes
---

지난주 금요일에 강의를 듣는데, 반 정도 듣자마자 전혀 머릿속에 안들어오기 시작했다. 수학과지만 통계는 고등학교 이후로 잡은 적이 없다보니 하나도 아는게 없어서 
내용이 이해가 안갔다. 이틀간 introduction, statistics가 들어간 책들을 살펴봤지만 소득이 없었는데 코세라에 검색해서 나온 <A href = "https://www.coursera.org/learn/stanford-statistics/home/welcome"> 이 강의 </A>가 아주 마음에 들었다. 
지금 데브코스에서 듣는 강의와 다루는 내용도 비슷하고 내용은 두배 정도 늘려놔서 이해하기 더 좋았다. 

오늘 진행된 라이브세션은 꽤 재미있었다. 나동빈 강사님이 진행하셨는데 검색해보니 꽤 많은 자료가 나오는 유명인이셨다. Adversarial attack에 대해 강의하셨다.  
Adversarial attack이란 딥러닝으로 학습된 모델을 대상으로 기존의 데이터와 아주 살짝 비뀐 데이터를 넣어 완전히 다른 결과가 나오게 유도하는 것을 말한다.  
그 방식으로 약한 노이즈를 주거나, 모양은 그대로 두면서 텍스쳐를 바꾸거나, 한 글자만 바꾸는 등의 다양한 방법이 있었다. 
그럼 그 노이즈 같은걸 어떻게 세팅할까? 하는 부분은 화이트박스 공격과 블랙박스 공격 두가지로 나뉘어진다.  
화이트박스 공격은 일반적으로 신경망이 학습하는 방식(경사하강법)을 반대로 사용하여 데이터를 아주 조금씩 변경하여 원하는 결과로 유도한다.  
블랙박스 공격은 두 가지 방법이 인상깊었는데 하나는 공격 목표인 모델과 비슷한 기능을 수행하는 모델을 직접 구현한 뒤, 그 모델에 대해 화이트박스 공격을 하고 그 데이터를 원래의 공격 목표에 적용하는 것이다. 이게 된다고 한다. 두번째로 
구역 A에 속한 데이터 a와 구역 B에 속한 데이터 b가 있으면 a를 조금씩 변형하고 b에 projection하는 것을 여러 방향으로 여러 번 시도하면 총 1만번 가량의 공격 시도로 목표를 달성할 수 있다고 한다.  
마지막으로 가장 인상깊었던 부분은 왜 adversarial attack이 가능한가? 하는 부분이였다. 특히 해당 원인을 robust와 non-robust로 나누었는데 robust는 일반적으로 인간이 인지 가능한 특징, non-robust는 인간이 보기엔 노이즈같은 특징들을 의미한다. 
그리고 adversarial attack은 non-robust쪽에 혼란을 줌으로써 정확도를 떨어트린다. 내가 이 부분이 인상깊은 이유 두가지가 있다. 
1. 기계는 인간이 인지 못하는 non-robust 정보들을 굉장히 중요하게 사용하고 있다. 따라서 기계가 완전히 새로운 지식을 만들어 낼 수 있을 거라 기대된다.  
2. 한편 기계는 인간이 주로 사용하는 robust 정보만으론 아직 충분한 정확도를 얻을 수 없다. 따라서 기계는 여전히 발전할 여지가 많이 남아 있다.

조금씩 막히는 부분이 생기면서 진도가 느려지고 있다. 공부해야할게 많다.